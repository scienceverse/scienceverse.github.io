[
  {
    "objectID": "structure_demo.html",
    "href": "structure_demo.html",
    "title": "Machine-Readable Structure Demo",
    "section": "",
    "text": "Here we will give an example of a basic machine-readable study description. This example uses JSON format, which could be converted into any structured format.\n\nStudy framework\nThe primary unit is a study, which contains categories where different study components will be described, such as the hypotheses, methods, data, and analyses. Here, we will focus on how we can specify the hypotheses and their links to the analyses.\n{\n    \"name\": \"Kinship and Prosocial Behaviour\",\n    \"authors\": []\n    \"hypotheses\": [],\n    \"methods\": [],\n    \"data\": [],\n    \"analyses\": []\n}\n\n\nHypotheses\nAt the most basic level, each hypothesis needs a verbal description. A study could contain multiple hypotheses, but our example contains only one. The example below contains placeholders for describing the criteria that will support or falsify the prediction, which we will fill in later in relation to the planned analysis output.\n    \"hypotheses\": [\n        {\n            \"description\": \"Cues of kinship will increase prosocial behaviour. Cues of kinship will be manipulated by morphed  facial self-resemblance. Prosocial behaviour will be measured by responses in the trust game. The prediction is that the number of trusting AND/OR reciprocating moves will be greater to self-resembling faces than to non-self-resembling faces.\",\n            \"criteria\": [],\n            \"support\": {},\n            \"falsify\": {}\n        }\n    ],\n\n\nAnalyses\nNext, we can specify the analyses. Here we will list the information necessary to run a t-test in R (indicated by the “software” value), but it would be possible to create machine-readable specifications for analyses using any other software package. We give the name of the function to be used and list the parameters. The notation “.data[kin]$trust_self” refers to the “trust_self” column of a dataset called “kin” to be specified later. Here, we have two analyses: a t-test comparing trusting moves to self-resembling faces (“trust_self”) versus non-self-resembling faces (“trust_non”), and a t-test comparing reciprocating moves to self-resembling faces (“recip_self”) versus non-self-resembling faces (“recip_non”).\n    \"analyses\": [\n        {\n            \"id\": \"trust_analysis\",\n            \"software\": \"`r R.Version()$version.string`\",\n            \"func\": \"t.test\",\n            \"params\": {\n                \"x\": \".data[kin]$trust_self\",\n                \"y\": \".data[kin]$trust_non\",\n                \"paired\": true,\n                \"conf.level\": 0.99\n            },\n            \"results\": {\n                \"conf.int\": []\n            }\n        },\n        {\n            \"id\": \"recip_analysis\",\n            \"software\": \"`r R.Version()$version.string`\",\n            \"func\": \"t.test\",\n            \"params\": {\n                \"x\": \".data[kin]$recip_self\",\n                \"y\": \".data[kin]$recip_non\",\n                \"paired\": true,\n                \"conf.level\": 0.99\n            },\n            \"results\": {\n                \"conf.int\": []\n            }\n        }\n    ]\n\n\nCriteria for support or falsification\nThe criteria for support and falsification now can be specified in relation to the planned analyses. Each criterion is specified with an ID, used to reference the criterion in the evaluation rules for support and falsification. We have two criteria that we need to check to determine support or falsification for each analysis: (1) Is the lower-bound of the 99% CI larger than 0? and (2) Is the upper bound of the 99% CI larger than 0.2 (our smallest effect size of interest)? After data collection and running analyses, each criterion’s “conclusion” value is changed from NULL to TRUE or FALSE.\n            \"criteria\": [\n                {\n                    \"id\": \"trust_lowbound\", \n                    \"analysis_id\": \"trust_analysis\",\n                    \"result\": \"conf.int[1]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0,\n                    \"conclusion\": NULL\n                },\n                {\n                    \"id\": \"trust_highbound\", \n                    \"analysis_id\": \"trust_analysis\",\n                    \"result\": \"conf.int[2]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0.2,\n                    \"conclusion\": NULL\n                },\n                {\n                    \"id\": \"recip_lowbound\", \n                    \"analysis_id\": \"recip_analysis\",\n                    \"result\": \"conf.int[1]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0,\n                    \"conclusion\": NULL\n                },\n                {\n                    \"id\": \"recip_highbound\", \n                    \"analysis_id\": \"recip_analysis\",\n                    \"result\": \"conf.int[2]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0.2,\n                    \"conclusion\": NULL\n                }\n            ]\n\n\nEvaluation of support or falsification\nThe values of “support” and “falsify” contain a verbal description of the criteria and an evaluation rule for determining what patterns of criteria support the hypothesis. In our example, the hypothesis is supported if both of the criteria are true for either the trust or reciprocation moves (“(trust_lowbound & trust_highbound) | (recip_lowbound & recip_highbound)”) and the hypothesis is falsified if the second criterion is false for both the trust and reciprocation moves (“!trust_highbound & !recip_highbound”). All other patterns of results are deemed inconclusive.\n            \"support\": {\n                \"description\": \"The hypothesis is supported if the 99% CI lower bound is greater than 0 and the 99% CI upper bound is greater than 0.2 (the SESOI) for either the trust or reciprocation moves.\",\n                \"evaluation\": \"(trust_lowbound & trust_highbound) | (recip_lowbound & recip_highbound)\"\n            },\n            \"falsify\": {\n                \"description\": \"The hypothesis is falsified if the 99% CI upper bound is smaller than 0.2 (the SESOI) for both trust and reciprocation.\",\n                \"evaluation\": \"!trust_highbound & !recip_highbound\"\n            }\n\n\nData\nWe can describe datasets using an existing codebook format from the PsychDS project by setting the “@context”, “@type” and “schemaVersion” values appropriately. Each dataset is given an ID for referencing in analysis scripts.\n    \"data\": [\n        {\n            \"id\": \"kin\",\n            \"@context\": \"https://schema.org/\",\n            \"@type\": \"Dataset\",\n            \"schemaVersion\": \"Psych-DS 0.1.0\",\n            \"variableMeasured\": [\n                {\n                    \"type\": \"PropertyValue\",\n                    \"unitText\": \"id\",\n                    \"name\": \"id\",\n                    \"description\": \"Subject ID\"\n                },\n                {\n                    \"type\": \"PropertyValue\",\n                    \"unitText\": \"trust_self\",\n                    \"name\": \"trust_self\",\n                    \"minValue\": 0,\n                    \"maxValue\": 3,\n                    \"description\": \"Number of trusting (P1) moves toward self-resembling faces\"\n                },\n                {\n                    \"type\": \"PropertyValue\",\n                    \"unitText\": \"trust_non\",\n                    \"name\": \"trust_non\",\n                    \"minValue\": 0,\n                    \"maxValue\": 3,\n                    \"description\": \"Number of trusting (P1) moves toward non-self-resembling faces\"\n                },\n                {\n                    \"type\": \"PropertyValue\",\n                    \"unitText\": \"recip_self\",\n                    \"name\": \"recip_self\",\n                    \"minValue\": 0,\n                    \"maxValue\": 3,\n                    \"description\": \"Number of reciprocating (P2) moves toward self-resembling faces\"\n                },\n                {\n                    \"type\": \"PropertyValue\",\n                    \"unitText\": \"recip_non\",\n                    \"name\": \"recip_non\",\n                    \"minValue\": 0,\n                    \"maxValue\": 3,\n                    \"description\": \"Number of reciprocating (P2) moves toward self-resembling faces\"\n                }\n            ]\n        }\n    ]\n\n\n\nFull JSON\n{\n    \"name\": \"Kinship and Prosocial Behaviour\",\n    \"authors\": []\n    \"hypotheses\": [\n        {\n            \"description\": \"Cues of kinship will increase prosocial behaviour. Cues of kinship will be manipulated by morphed  facial self-resemblance. Prosocial behaviour will be measured by responses in the trust game. The prediction is that the number of trusting AND/OR reciprocating moves will be greater to self-resembling faces than to non-self-resembling faces.\",\n            \"criteria\": [\n                {\n                    \"id\": \"trust_lowbound\", \n                    \"analysis_id\": \"trust_analysis\",\n                    \"result\": \"conf.int[1]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0,\n                    \"conclusion\": NULL\n                },\n                {\n                    \"id\": \"trust_highbound\", \n                    \"analysis_id\": \"trust_analysis\",\n                    \"result\": \"conf.int[2]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0.2,\n                    \"conclusion\": NULL\n                },\n                {\n                    \"id\": \"recip_lowbound\", \n                    \"analysis_id\": \"recip_analysis\",\n                    \"result\": \"conf.int[1]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0,\n                    \"conclusion\": NULL\n                },\n                {\n                    \"id\": \"recip_highbound\", \n                    \"analysis_id\": \"recip_analysis\",\n                    \"result\": \"conf.int[2]\", \n                    \"operator\": \"&gt;\", \n                    \"comparator\": 0.2,\n                    \"conclusion\": NULL\n                }\n            ],\n            \"support\": {\n                \"description\": \"The hypothesis is supported if the 99% CI lower bound is greater than 0 and the 99% CI upper bound is greater than 0.2 (the SESOI) for either the trust or reciprocation moves.\",\n                \"evaluation\": \"(trust_lowbound & trust_highbound) | (recip_lowbound & recip_highbound)\"\n            },\n            \"falsify\": {\n                \"description\": \"The hypothesis is falsified if the 99% CI upper bound is smaller than 0.2 (the SESOI) for both trust and reciprocation.\",\n                \"evaluation\": \"!trust_highbound & !recip_highbound\"\n            }\n        }\n    ],\n    \"methods\": [],\n    \"data\": [],\n    \"analyses\": [\n        {\n            \"id\": \"trust_analysis\",\n            \"software\": \"`r R.Version()$version.string`\",\n            \"func\": \"t.test\",\n            \"params\": {\n                \"x\": \".data[kin]$trust_self\",\n                \"y\": \".data[kin]$trust_non\",\n                \"paired\": true,\n                \"conf.level\": 0.99\n            },\n            \"results\": {\n                \"conf.int\": []\n            }\n        },\n        {\n            \"id\": \"recip_analysis\",\n            \"software\": \"`r R.Version()$version.string`\",\n            \"func\": \"t.test\",\n            \"params\": {\n                \"x\": \".data[kin]$recip_self\",\n                \"y\": \".data[kin]$recip_non\",\n                \"paired\": true,\n                \"conf.level\": 0.99\n            },\n            \"results\": {\n                \"conf.int\": []\n            }\n        }\n    ]\n}"
  },
  {
    "objectID": "metadatalist.html",
    "href": "metadatalist.html",
    "title": "Meta-data Projects and Tools",
    "section": "",
    "text": "This page lists projects and tools with the aligned goals of producing and organising machine-readable meta-data. Many of these were identified by participants in the at the SIPS 2019 Open Documentation Hackathon (Projects group Google Doc)."
  },
  {
    "objectID": "metadatalist.html#tools",
    "href": "metadatalist.html#tools",
    "title": "Meta-data Projects and Tools",
    "section": "Tools",
    "text": "Tools\n\nCodebook\nEasily automate the following tasks to describe data frames: - summarise the distributions, and labelled missings of variables graphically and using descriptive statistics - for surveys, compute and summarise reliabilities (internal consistencies, retest, multilevel) for psychological scales, - combine this information with metadata (such as item labels and labelled values) that is derived from R attributes.\n\n\nformr survey framework\nChain simple forms / surveys into longer runs using the power of R to generate pretty feedback and complex designs\n\n\nexperimentum\nWeb-based platform for psychology studies and their management.\n\n\njustifier\nThe justifier package offers a flexible compromise that enables thorough documentation that is both human- and machine-readable. This allows learning from the decisions taken in one or multiple projects, and making clear where decisions were based on strong versus weak justifications. What exactly comprises a strong or weak justification is of course greatly dependent on domain and context, and justifier provides for this. Justification frameworks for two specific cases have been provided in the vignettes ‘justifier in behavior change intervention development’ and ‘justifier in study design’.\n\n\nProlific\nEnabling fast, reliable and large-scale data collection by connecting researchers with participants around the world.\n\n\nMetafor\nThe metafor package is a free and open-source add-on for conducting meta-analyses with the statistical software environment R. The package consists of a collection of functions that allow the user to calculate various effect size or outcome measures, fit fixed-, random-, and mixed-effects models to such data, carry out moderator and meta-regression analyses, and create various types of meta-analytical plots."
  },
  {
    "objectID": "metadatalist.html#archivesdatabases",
    "href": "metadatalist.html#archivesdatabases",
    "title": "Meta-data Projects and Tools",
    "section": "Archives/Databases",
    "text": "Archives/Databases\n\nPsyArXiv\nA free preprint service for the psychological sciences.\n\n\nRe3Data\nre3data.org is a global registry of research data repositories that covers research data repositories from different academic disciplines. It presents repositories for the permanent storage and access of data sets to researchers, funding bodies, publishers and scholarly institutions. re3data.org promotes a culture of sharing, increased access and better visibility of research data.\n\n\nCochrane library\nA collection of databases that contain different types of high-quality, independent evidence to inform healthcare decision-making.\n\n\nClinicaltrials.gov\nClinicalTrials.gov is a database of privately and publicly funded clinical studies conducted around the world.\n\n\nGESIS Data Search\nSearch for social and economic research data across a diverse portfolio of data repositories and metadata services.\n\n\nGoogle Dataset Search\nDataset Search enables users to find datasets stored across the Web through a simple keyword search. The tool surfaces information about datasets hosted in thousands of repositories across the Web, making these datasets universally accessible and useful.\n\n\nZPID Test archive\n\n\nGESIS ZIS (Collection of Items and Scales for the Social Sciences\nZIS stands for the Collection of Items and Scales for the Social Sciences (German: ZIS - Zusammenstellung sozialwissenschaftlicher Items und Skalen). ZIS contains more than 250 instruments, e.g. items for measuring political attitudes or personality. Each instrument documentation includes the instrument itself and information about the instrument’s theoretical background, development, and quality criteria. The ZIS instruments are sorted by topics."
  },
  {
    "objectID": "metadatalist.html#curation",
    "href": "metadatalist.html#curation",
    "title": "Meta-data Projects and Tools",
    "section": "Curation",
    "text": "Curation\n\nCurate Science\nA platform for researchers to label and link the transparency and replication of their research.\n\n\nhypothes.is\nUse Hypothesis to hold discussions, read socially, organize your research, and take personal notes.\n\n\nZenodo\nShare research outputs, get a DOI, curate your own digital repository."
  },
  {
    "objectID": "metadatalist.html#standardsspecification",
    "href": "metadatalist.html#standardsspecification",
    "title": "Meta-data Projects and Tools",
    "section": "Standards/Specification",
    "text": "Standards/Specification\n\nDOI\nThe DOI system provides a technical and social infrastructure for the registration and use of persistent interoperable identifiers, called DOIs, for use on digital networks.\n\n\nORCID\nORCID is part of the wider digital infrastructure needed for researchers to share information on a global scale. We enable transparent and trustworthy connections between researchers, their contributions, and affiliations by providing an identifier for individuals to use with their name as they engage in research, scholarship, and innovation activities.\n\n\nBrain Imaging Data Structure (BIDS)\nNeuroimaging experiments result in complicated data that can be arranged in many different ways. So far there is no consensus how to organize and share data obtained in neuroimaging experiments. Even two researchers working in the same lab can opt to arrange their data in a different way. Lack of consensus (or a standard) leads to misunderstandings and time wasted on rearranging data or rewriting scripts expecting certain structure. Here we describe a simple and easy to adopt way of organizing neuroimaging and behavioral data.\n\n\nNeuroimaging data model (NIDM)\nThe Neuroimaging Data Model (NIDM) is a collection of specification documents that define extensions the W3C PROV standard for the domain of human brain mapping. NIDM uses provenance information as means to link components from different stages of the scientific research process from dataset descriptors and computational workflow, to derived data and publication.\n\n\nDDI\nThe Data Documentation Initiative (DDI) is an international standard for describing the data produced by surveys and other observational methods in the social, behavioral, economic, and health sciences. DDI is a free standard that can document and manage different stages in the research data lifecycle, such as conceptualization, collection, processing, distribution, discovery, and archiving.\n\n\nPSYCH-DS\nFAIR project organisation: Work with your data in formats that make your life easy and help you document and share with others. Human friendly, machine readable: Designed for individual scientists to generate machine-readable datasets and metadata. Community driven: Building consensus for how we structure/document our data so we can find and extend each others’ work.\n\n\n(CRED-nf checklist)[https://psyarxiv.com/nyx84/]\nThis checklist is intended to encourage robust experimental design and clear reporting for clinical and cognitive-behavioural neurofeedback experiments."
  },
  {
    "objectID": "metadatalist.html#frameworksorganisations",
    "href": "metadatalist.html#frameworksorganisations",
    "title": "Meta-data Projects and Tools",
    "section": "Frameworks/Organisations",
    "text": "Frameworks/Organisations\n\nOSF\nOnline framework for organising and sharing research components.\n\n\nDublin Core Metadata Initiative (DCMI)\nAn organization supporting innovation in metadata design and best practices across the metadata ecology. DCMI’s activities include:\n\nwork on architecture and modelling\ndiscussions and collaborative work in DCMI Communities and DCMI Task Groups\nglobal conferences, meetings and workshops\neducational efforts to promote widespread acceptance of metadata standards and best practices.\n\n\n\nZPID DataWiz\nAutomated Assistant for the Management of Psychological Research Data. DataWiz supports researchers in planning their data management before the project starts and in managing their research data during the project. It provides functions for data preparation, documentation and archiving, as well as a digital collaborative working environment for you and your team. In this way, project DataWiz pursues the following strategic goals:\n\nreduce the time spent on research data management by researchers,\nrelieve the load on research data repositories by increasing quality of deposited data,\nestablish domain-adequate research data management and open science practices in psychology.\n\n\n\n(workflowr)[https://jdblischak.github.io/workflowr/]\nThe workflowr R package helps researchers organize their analyses in a way that promotes effective project management, reproducibility, collaboration, and sharing of results. Workflowr combines literate programming (knitr and rmarkdown) and version control (Git, via git2r) to generate a website containing time-stamped, versioned, and documented results.\n\n\nfmriprep\nfmriprep is a functional magnetic resonance imaging (fMRI) data preprocessing pipeline that is designed to provide an easily accessible, state-of-the-art interface that is robust to variations in scan acquisition protocols and that requires minimal user input, while providing easily interpretable and comprehensive error and output reporting. It performs basic processing steps (coregistration, normalization, unwarping, noise component extraction, segmentation, skullstripping etc.) providing outputs that can be easily submitted to a variety of group level analyses, including task-based or resting-state fMRI, graph theory measures, surface or volume-based statistics, etc.\n\n\nmetaBrainz & musicBrainz\nThe MetaBrainz Foundation is a non-profit that believes in free, open access to data. It has been set up to build community maintained databases and make them available in the public domain or under Creative Commons licenses. Our data is mostly gathered by volunteers and verified by peer review to ensure it is consistent and correct. All non-commercial use of this data is free, but commercial users are asked to support us in order to help fund the project. We encourage all data users to contribute to the data gathering process so that our data can be as comprehensive as possible."
  },
  {
    "objectID": "metadatalist.html#formats",
    "href": "metadatalist.html#formats",
    "title": "Meta-data Projects and Tools",
    "section": "Formats",
    "text": "Formats\n\nSchema.org\nSchema.org is a collaborative, community activity with a mission to create, maintain, and promote schemas for structured data on the Internet, on web pages, in email messages, and beyond. Schema.org vocabulary can be used with many different encodings, including RDFa, Microdata and JSON-LD. These vocabularies cover entities, relationships between entities and actions, and can easily be extended through a well-documented extension model.\n\n\nPortable Format for Analytics (PFA)\nIt provides a common interface to safely deploy analytic workflows across environments, from embedded systems to distributed data centers. PFA is an emerging standard for statistical models and data transformation engines. PFA combines the ease of portability across systems with algorithmic flexibility: models, pre-processing, and post-processing are all functions that can be arbitrarily composed, chained, or built into complex workflows. PFA may be as simple as a raw data transformation or as sophisticated as a suite of concurrent data mining models, all described as a JSON or YAML configuration file."
  },
  {
    "objectID": "metadatalist.html#other-resources",
    "href": "metadatalist.html#other-resources",
    "title": "Meta-data Projects and Tools",
    "section": "Other Resources",
    "text": "Other Resources\n\nA Practical Guide for Transparency in Psychological Science\nHere we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.)."
  },
  {
    "objectID": "posts/2025-10-28_festival-of-ai.html",
    "href": "posts/2025-10-28_festival-of-ai.html",
    "title": "Research Literature Demonstration: Festival of Data Science and AI",
    "section": "",
    "text": "Lisa DeBruine gave a Papercheck workshop at the Festival of Data Science and AI at the University of Glasgow in Scotland.\nWe had an in-depth demonstration of how Papercheck might be useful for systematic reviews."
  },
  {
    "objectID": "posts/2025-07-23_open-research-summer-school.html",
    "href": "posts/2025-07-23_open-research-summer-school.html",
    "title": "Open Research Summer School",
    "section": "",
    "text": "Cristian Mesquida gave a Papercheck workshop at the Open Research Summer School at King’s College London.\nHe explained the rationale for automated checks, and walked the audience through some of the Papercheck modules.\nYou can find the workshop materials on the Open Science Framework: osf.io/y4s76."
  },
  {
    "objectID": "posts/2025-07-27_intro-blog.html",
    "href": "posts/2025-07-27_intro-blog.html",
    "title": "Introductory Blog Posts",
    "section": "",
    "text": "We have started the project with a series of blog posts introducing the software package called papercheck that forms the basis for the project.\n\nIntroducing Papercheck\nAutomatically Checking Journal Article Reporting Standards\nChecking Data and Code in Repositories\nRetrieving Planned Sample Sizes\nEasily Download OSF Files\n\nThe posts attracted attention of researchers primarily in the meta science community. We are exploring new collaborations and continue to look for similar initiatives, tools and verification data. If you know of any new developments we should know about, please let us know.\nOver the summer we will be developing and validating new modules for the software, focusing primarily on the field of experimental psychology. After the summer we will start with the deliberative conversations in other data communities."
  },
  {
    "objectID": "posts/2025-10-06_launch.html",
    "href": "posts/2025-10-06_launch.html",
    "title": "Transparency Check Launch Event",
    "section": "",
    "text": "On Monday October 6, 2025, 14.00-17.00 in the Aurora room of the main building at VU Amsterdam, we had the Kick-off meeting for Transparency Check.\n\nProgram\n\nA plenary keynote by Daniel Lakens and Rene Bekkers, introducing the project\nA plenary illustration of transparency indicators that you can already assess with the papercheck package by Lisa DeBruine\nParallel sessions: A. Deliberative conversation on transparency indicators, providing input on which aspects of research reports we should develop modules for; B. Software testing and development workshop, providing a user experience and suggestions for improvement of existing modules\nPlenary closing by Rene Bekkers\n\nBecause of budget limitations, we cannot reimburse travel and accommodation. We will provide a link to an online meeting room if you cannot make it to Amsterdam."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ScienceVerse",
    "section": "",
    "text": "The increasingly digital workflow in science has made it possible to share almost all aspects of the research cycle, from pre-registered analysis plans and study materials to the data and analysis code that produce the reported results. Although the growing availability of research output is a positive development, most of this digital information is in a format that makes it difficult to find, access, and reuse. A major barrier is the lack of a framework to concisely describe every component of research in a machine-readable format: A grammar of science."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "ScienceVerse",
    "section": "Software",
    "text": "Software\nWe are developing software to achieve our goals of generating and processing machine-readable descriptions to facilitate archiving studies, pre-registration, finding variables and measures used in other research, meta-analyses of studies, finding and re-using datasets in other ways, and assessing research outputs for best practices.\nOur current focus is on developing MetaCheck, which aims to screen scientific manuscripts to identify potential issues or areas for improvement and guide researchers in adopting best practices. It can also assist with processing large numbers of papers for metascientific enquiry.\n\n\n\n\n\n\n\n\n\nscienceverse generates machine-readable descriptions of research objects\n\n\n\n\n\n\n\nmetacheck provides extendable tools for checking research outputs for best practices\n\n\n\n\n\n\n\nfaux simulates data from descriptive statistics"
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "ScienceVerse",
    "section": "Funding",
    "text": "Funding\nThis project is partially funded by the TDCC Research Transparency Check Project, co-led by René Bekkers and Daniël Lakens. Aspects of the project are also funded by a Vici grant to Daniël Lakens."
  },
  {
    "objectID": "index.html#core-team",
    "href": "index.html#core-team",
    "title": "ScienceVerse",
    "section": "Core Team",
    "text": "Core Team\nScienceverse is a collaborative project led by Lisa DeBruine (University of Glasgow) and Daniël Lakens (Technical University Eindhoven). Other contributors are Cristian Mesquida (postdoc, TUE), Jakub Werner (RA, TUE), Hadeel Khawatmy (RA, TUE), Lavinia Ion (RA, TUE), René Bekkers (Transparency Check co-PI, VU Amsterdam), and Max Littel (RA, VU).\n\n\n\n\n\n\n\n\n\nLisa DeBruine\n\n\n\n\n\n\n\nDaniël Lakens\n\n\n\n\n\n\n\nCristian Mesquida\n\n\n\n\n\n\n\nJakub Werner\n\n\n\n\n\n\n\n\n\nHadeel Khawatmy\n\n\n\n\n\n\n\nLavinia Ion\n\n\n\n\n\n\n\nRené Bekkers\n\n\n\n\n\n\n\nMax Littel"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "ScienceVerse",
    "section": "Publications",
    "text": "Publications\nLakens, D., & DeBruine, L. M. (2020). Improving Transparency, Falsifiability, and Rigour by Making Hypothesis Tests Machine Readable. Advances in Methods and Practices in Psychological Science. 2021;4(2). doi:10.1177/2515245920970949 (more accessible preprint version)"
  },
  {
    "objectID": "posts/2025-10-27_ai-ethics.html",
    "href": "posts/2025-10-27_ai-ethics.html",
    "title": "Using AI Ethically in Research: Festival of Data Science and AI",
    "section": "",
    "text": "Lisa DeBruine spoke about Papercheck in a panel discussion about Ethics in AI at the Festival of Data Science and AI at the University of Glasgow in Scotland.\nSlides"
  },
  {
    "objectID": "posts/2025-07-25_glasgow-seminar.html",
    "href": "posts/2025-07-25_glasgow-seminar.html",
    "title": "Glasgow Seminar on Papercheck",
    "section": "",
    "text": "Lisa DeBruine presented Papercheck in a talk for the University of Glasgow School of Psychology & Neuroscience summer seminar series.\nAccess the slides"
  },
  {
    "objectID": "posts/2025-07-25_glasgow-seminar.html#abstract",
    "href": "posts/2025-07-25_glasgow-seminar.html#abstract",
    "title": "Glasgow Seminar on Papercheck",
    "section": "Abstract",
    "text": "Abstract\nIn this talk, I will introduce Papercheck, a new tool that leverages text search, code, and large language models to extract and supplement information from scientific documents (including manuscripts, submitted or published articles, or preregistration documents) and provides automated suggestions for improvement.\nInspired by practices in software development, where automated checks (e.g., CRAN checks for R packages) are used to identify issues before release, Papercheck aims to screen scientific manuscripts to identify potential issues or areas for improvement and guide researchers in adopting best practices. It can also assist with processing large numbers of papers for metascientific enquiry."
  },
  {
    "objectID": "posts/2025-07-02_metascience-unconference.html",
    "href": "posts/2025-07-02_metascience-unconference.html",
    "title": "MetaScience Unconference",
    "section": "",
    "text": "Lisa DeBruine gave an impromptu Papercheck workshop in the unconference section of the MetaScience conference at UCL in London.\nAbout a dozen people attended and we discussed how Papercheck could help with their metascientific resaerch workflows."
  },
  {
    "objectID": "sips2019.html",
    "href": "sips2019.html",
    "title": "SIPS 2019 Hackathon on Open research documentation",
    "section": "",
    "text": "The goal of scienceverse is to generate and process machine-readable study descriptions to facilitate archiving studies, pre-registering studies, finding variables and measures used in other research, meta-analyses of studies, and finding and re-using datasets in other ways. Explicitly documenting hypotheses, materials, methods/procedure, data, and analysis code facilitates cumulative science. It is often difficult to exactly recover some of these aspects, and usually impossible to automate this process. We will present a preliminary version of a framework (with an associated R package and shiny app) for organising this information in a machine-readable way and automatically producing human-readable summaries. This framework can integrate with other projects to document study planning decisions (e.g., justifier), data (e.g., Psych-DS, codebook), and methods (e.g., experimentum, formR).\nWe have 4 main goals in this project, which we might examine collaboratively, or in small subgroups.\n\nImagine a fully developed Grammar of Science exists. All aspects of a study you need are machine readable. What would you want to use it for? Which use-cases can we identify?\nWhat do we need to get to this fully developed Grammar of Science? Which aspects of the research cycle and research output do we need to develop a Grammar for? For example, how can we develop a Grammar for paradigms used in psychology?\nWhat are the potential roadblocks to realizing a Grammar of Science that can be widely implemented? What is needed to resolve these roadblocks?\nHow does Scienceverse integrate with other ongoing projects at SIPS? Is it possible for Scienceverse to facilitate these other projects? E.g., machine readable codebooks, better preregistration documents, Psych-DS, etc. Throughout SIPS, tag any projects you run into that Scienceverse should interact with!"
  },
  {
    "objectID": "sips2019.html#abstract",
    "href": "sips2019.html#abstract",
    "title": "SIPS 2019 Hackathon on Open research documentation",
    "section": "",
    "text": "The goal of scienceverse is to generate and process machine-readable study descriptions to facilitate archiving studies, pre-registering studies, finding variables and measures used in other research, meta-analyses of studies, and finding and re-using datasets in other ways. Explicitly documenting hypotheses, materials, methods/procedure, data, and analysis code facilitates cumulative science. It is often difficult to exactly recover some of these aspects, and usually impossible to automate this process. We will present a preliminary version of a framework (with an associated R package and shiny app) for organising this information in a machine-readable way and automatically producing human-readable summaries. This framework can integrate with other projects to document study planning decisions (e.g., justifier), data (e.g., Psych-DS, codebook), and methods (e.g., experimentum, formR).\nWe have 4 main goals in this project, which we might examine collaboratively, or in small subgroups.\n\nImagine a fully developed Grammar of Science exists. All aspects of a study you need are machine readable. What would you want to use it for? Which use-cases can we identify?\nWhat do we need to get to this fully developed Grammar of Science? Which aspects of the research cycle and research output do we need to develop a Grammar for? For example, how can we develop a Grammar for paradigms used in psychology?\nWhat are the potential roadblocks to realizing a Grammar of Science that can be widely implemented? What is needed to resolve these roadblocks?\nHow does Scienceverse integrate with other ongoing projects at SIPS? Is it possible for Scienceverse to facilitate these other projects? E.g., machine readable codebooks, better preregistration documents, Psych-DS, etc. Throughout SIPS, tag any projects you run into that Scienceverse should interact with!"
  },
  {
    "objectID": "sips2019.html#resources",
    "href": "sips2019.html#resources",
    "title": "SIPS 2019 Hackathon on Open research documentation",
    "section": "Resources",
    "text": "Resources\n\nMain Google Doc\nProjects group Google Doc\nNeeds Google Doc\nOSF\nSlack channel\n\nThe links above provide the raw materials created during the hackathon. Below, we summarise the discussion."
  },
  {
    "objectID": "sips2019.html#use-cases",
    "href": "sips2019.html#use-cases",
    "title": "SIPS 2019 Hackathon on Open research documentation",
    "section": "Use Cases",
    "text": "Use Cases\nImagine a fully developed Grammar of Science exists. All aspects of a study you need are machine readable. What would you want to use it for? Which use-cases can we identify?\n\nContributors\n\nAnna Szabelska (aszabelska01@qub.ac.uk)\nAleksandra Tolopilo (atolopilo@swps.edu.pl)\nBjørn Sætrevik (bjorn.satrevik@uib.no)\nEkaterina (Katia) Damer ek.damer@gmail.com\nMichal Olszanowski (molszanowski@swps.edu.pl)\nMartin Müller (martin.mueller82@univie.ac.at)\nKohinoor Monish Darda (k.darda@bangor.ac.uk)\nConstantin Yves Plessen(yves.plessen@me.com)\nSau-Chin Chen; pmsp96@gmail.com\nIlse Pit; ilsepit@gmail.com; @ilsepit\nPierpaolo Primoceri pierpaolo.primoceri@uzh.ch\nRuud Hortensius ruud.hortensius@glasgow.ac.uk\nZoltan Kekecs (kekecs.zoltan@gmail.com)\nDavid Moreau d.moreau@auckland.ac.nz\nKristina Wiebels kwie508@aucklanduni.ac.nz\n\n\n\nEvaluating literature:\n\nSimplifying and accelerating review processes\nGive machine recommendations for reviewers - e.g.: check this section, because it looks different from the others, or that it looks fine.\nIntegrating of behavioural/neuroimaging/etc. data, methods, databases etc with published ‘article’\nEase the evaluation of manuscripts, which accelerates the review process\nLinks between behavioural, neuroimaging and other kinds of data and univariate and multivariate analyses\nA standardized taxonomy for keywords, cover all psy research in non-overlapping categories\nWho do I need to cite in my paper?\nWho should I recommend as a reviewer for my manuscript?\n\n\n\nAccelerate the writing of papers\n\nAutomatic answers to questions you have, e.g. what is the effect of x on y? And get an answer based on the database, including reliability score\nAutomated manuscript write-up based on the pre-registered hypotheses and analysis plan (machine-recommended language for the results section of the manuscript)\nSimilar to “fMRI prep”\n\n\n\nResearch synthesis & meta analysis\n\nMake data (sets) accessible and searchable as a whole by topics/variables\nAutomated meta-analyses / continuously updating meta-analyses\nWe can have automated meta-meta-analyses on more meta-subjects\nFuture proofing meta-analyses / meta-meta analyses\nAll transformations / formulas need to be attached as meta-data\nDecisions rules in case of deviations from the analysis plan need to be specified (i.e. categorical moderator is changed from 4 to 3 categories because not enough observations were obtained in one category)\n\n\n\nCollaboration / planning\n\nBe able to Google data sets\nWho have looked at the relationship between these variables before?\n\nWhen considering new projects:\nWhen evaluating grant proposals for novelty\n\nCreate study blobs - linking up unrelated studies which use the same variables\nIncluding future research ideas/hypotheses that are also accessible / recommended future research questions\nSuggesting the design based on the claimed hypotheses\nGood for big collaborations\nTime stamped workflow (e.g., integrated with workflowr)\nCompare semantic information: instructions etc\n\n\n\nScicom\nScience education & communication, community, stakeholders\n\nBetter science education, e.g. this could broaden students’ scope / perspective on what data already exists and what hypotheses one might test\nBetter science communication and dissemination in applied settings, e.g. scienceverse could make it easier for journalists or politicians to interpret evidence and its robustness\nOne format to teach students\nSearch enginge where people from the public can enter research questions, and a bot will convert the question to research hypothesis and will search the database for compatible stuides, do a quick meta-analyis and answer the question\n\n\n\nPre-registration\n\nA shiny-app for (quick/simple?) pre-registrations in machine readable form.\n\nStart with simple but formalized predictions.\nSuggest as one of the OSF pre-reg options?\nDoes not need to wait for “the perfect future”\n\nBetter comparison between different pre-registrations/hypotheses of similar studies/investigations\nIntegrate the preregistration templates\n\nInclude power analyses (if not already possible)\n\n\n\nTheoretical development\n\nWork towards “a grand unified theory” understanding of psychology\nMachine readable findings would allow us to compile all findings on a given subject, compare contributions\nIterate faster between developing theory and testing hypotheses\n\n\n\nAnalysis\n\nPre-processing:\n\ncleaning data\nexactly when items and variables removed\noutliers\nwrong values\nCrohnbach’s alpha is wrong\n\nContainers: standardized ways of analysing (also preprocessing) data\nRunning other’s hypotheses on your data automatically\n\n\n\nExperimental design:\n\nStandardize experimental manipulation methods\nReuse experiments; psychopy (scripts) info etc\nRecycle stimuli presentation script\nRecycle questionnaires/experiment info\nIncreases reproducibility\n\n\n\nReliability / robustness:\n\nEvaluate how robust were the analyses?\nAdding meta-data for multiverse and/or specification curve analysis?\nWhat other decisions/analyses could have been done?\nAutomated robustness check based on the literature\nLimit researcher degrees of freedom and avoid p-hacking\n\n\n\nIntegrations\n\nR Markdown *Prolific reduce research redundancy (so researchers don’t have to collect the same data over and over again, but can see where there are gaps in the data)"
  },
  {
    "objectID": "sips2019.html#needs",
    "href": "sips2019.html#needs",
    "title": "SIPS 2019 Hackathon on Open research documentation",
    "section": "Needs",
    "text": "Needs\nWhat do we need to get to this fully developed Grammar of Science? Which aspects of the research cycle and research output do we need to develop a Grammar for? For example, how can we develop a Grammar for paradigms used in psychology?\nThis groups’ thoughts are pretty complex and can best be viewed at the Google Doc until we have a chance to synthesise things.\n\nContributors\n\nDavid Moreau, The University of Auckland, d.moreau@auckland.ac.nz\nKristina Wiebels, The University of Auckland, kwie508@aucklanduni.ac.nz\nIan Hussey, Ghent University, @ianhussey ian.hussey@ugent.be\nGinette Lafit, KU Leuven, ginette.lafit@kuleuven.be\nLiam Satchell @lpsatchell University of Winchester, liam.satchell@winchester.ac.uk\nTanja Burgard, ZPID, tb@leibniz-psychology.org\nElena SIxtus, Uni Potsdam, esixtus@uni-potsdam.de\nDaniel Toribio-Florez, toribio-florez@coll.mpg.de\nShannon McNee, University of Glasgow, shannon.mcnee18@gmail.com\nAnna Lohmann, anna@lohmann-web.net\nEllie Hassan, University of Exeter, eh616@exeter.ac.uk\nGabriela Hofer, University of Graz, gabriela.hofer@uni-graz.at\nFranziska Stanke, University of Münster, fstanke@wwu.de\nDivya Seernani, divya.seernani@gmail.com\nElise Gould, elise.gould@unimelb.edu.au"
  },
  {
    "objectID": "sips2019.html#roadblocks",
    "href": "sips2019.html#roadblocks",
    "title": "SIPS 2019 Hackathon on Open research documentation",
    "section": "Roadblocks",
    "text": "Roadblocks\nWhat are the potential roadblocks to realizing a Grammar of Science that can be widely implemented? What is needed to resolve these roadblocks?\n\nContributors\n\n\nMotivations\n\nThere needs to be a reason for researchers (not just meta-scientists) to use the scienceverse. This could be if the scienceverse becomes a useful database of results; one that researchers would search before conducting their studies (in addition to, or as part of, Google Scholar and other academic search engines).\n\n\n\nMissing essentials\n\nRegistration documents must clearly specify the primary and secondary hypotheses.\nThe package needs to incorporate the ability to separate and specify changes to preregistered analysis protocols.\nContainerisation will be required to make the results from scienceverse hypotheses fully reproducible.\n\n\n\nComplexity/simplicity\n\nThe scienceverse may not be for everyone at first. Does this limit its utility?\nResearchers will find it difficult to specify their hypotheses in advance.\nIncluding the results of more complex statistical analyses may be more difficult, but it is possible: the key parameters just need to be included in a list.\nThere is an extra step involved for those conducting their analyses outside of R. This could be another barrier to entry, but may be overcome by the use of easy-to-complete online apps.\nTo work, the specification must be very flexible. This will require a great deal of development.\nPeople use things because of their cool tools! Their extra functionality. If the scienceverse is linked to other useful functionality then people will be more likely to use it.\nIntegration with other open source software would make the scienceverse easier to use (e.g., JASP, jamovi).\n\n\n\nStandarisation vs. flexibility\n\nThe centralisation of the project could make it difficult to implement and difficult to get researchers on board who do not use other open science websites and tools.\nNaming analysis parameters in a standardised way will facilitate the use of the scienceverse, including for meta-analysis. But the scienceverse must work well with non-standardised output for it to be widely adopted and easy to use (currently possible but more difficult?). This link may be possible within json files.\n\n\n\nOptimism\n\nResearchers may be motivated to use the scienceverse because it could help them to plan their analyses by helping them to simulate data based on their hypotheses and data structure.\nThis could help researchers to find and work with the particular measurement tools they are interested in using or analysing data from (can also include delivery variables, e.g. online vs. face-to-face, standard question order vs. a randomised order).\nA book on experimental design using the scienceverse (and incorporation in teaching) may be required to get students and academic faculty to use it."
  },
  {
    "objectID": "sips2019.html#aligned-projects",
    "href": "sips2019.html#aligned-projects",
    "title": "SIPS 2019 Hackathon on Open research documentation",
    "section": "Aligned Projects",
    "text": "Aligned Projects\nHow does Scienceverse integrate with other ongoing projects at SIPS? Is it possible for Scienceverse to facilitate these other projects? E.g., machine readable codebooks, better preregistration documents, Psych-DS, etc. Throughout SIPS, tag any projects you run into that Scienceverse should interact with!\n\nContributors\n\nrobert.t.thibault@gmail.com @rt_thibault\nerikbpanderson@gmail.com @ebanders\nJohannes Breuer, johannes.breuer@gesis.org, @MattEagle09\nmayer@psychologie.uzh.ch\nStephan Heunis\nThomas Richardson\nElise Gould\nZita Mayer\n\nThe list of Meta-data Projects and Tools is derived from the list generated by this group.\n\n\nIdeas\n\nEssential versus encouraged documentation\nAre the researchers responsible for inputting all the information? Or, should we involve librarians or others to fill in the grammar field?\nWhat if we still have meta-analyses where those interested in a specific questions fill in all the grammar? This takes the extra workload off the original researchers (find the right balance)\nCan we make a common format platform for meta-analysts and systematic reviewers to upload their findings too, so that the reviews can be continued into the future?\nThis could incentivize the release of old data\nCunningham’s law: the quickest way to find the right answer (online) is to post the wrong answer.\nCould journals (esp. community journals) implement checklists to help achieve a grammar of science (eg. OHBM–organization for human brain mapping, is using the COBIDAS checklist)\nWhat about journals like Psychological Science, or APA journals…\nPossibility to generate some visual output/summary of the study/project structure/design (that could be included in presentations or publications)?\nThe specification of all study parts (esp. In the analysis) for complex designs might be challenging (for example, longitudinal studies, complex sampling strategies, multilevel models, etc.)\nDocumentation ideally for whole research lifecycle (for a nice visualization of the research lifecycle see https://how-to-open.science/)\nIt should “threaten” to constrain exploratory research"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-10-28\n\n\nUsing AI Ethically in Research: Festival of Data Science and AI\n\n\nLisa DeBruine\n\n\n\n\n2025-10-28\n\n\nResearch Literature Demonstration: Festival of Data Science and AI\n\n\nLisa DeBruine\n\n\n\n\n2025-10-06\n\n\nTransparency Check Launch Event\n\n\nRene Bekkers\n\n\n\n\n2025-07-27\n\n\nIntroductory Blog Posts\n\n\nDaniël Lakens\n\n\n\n\n2025-07-25\n\n\nGlasgow Seminar on Papercheck\n\n\nLisa DeBruine\n\n\n\n\n2025-07-23\n\n\nOpen Research Summer School\n\n\nCristian Mesquida\n\n\n\n\n2025-07-02\n\n\nMetaScience Unconference\n\n\nLisa DeBruine\n\n\n\n\n\nNo matching items"
  }
]